{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Variance Explained by a PCA Model\n",
    "\n",
    "This notebook will show how to measure the amount of variance that can be explained by the top $k$ principal components in a Principal Component Analysis (PCA) model. This technique is used to pick the number of lower dimensional space dimensions when performing dimensionality reduction using PCA.\n",
    "\n",
    "For the purposes of this demonstration, we will use the wine dataset from the UCI Machine Learning Repository, found at https://archive.ics.uci.edu/ml/datasets/Wine. This demo was inspired by Sebastian Raschka's demo found at https://plot.ly/ipython-notebooks/principal-component-analysis/.\n",
    "\n",
    "Just as there are multiple methods to compute a PCA model, we will show two different ways to measure the percent of explained variance in the model. This percentage is computed from the eigenvalues obtained after the eigendecomposition of the covariance matrix step in PCA. In short, the eigenvectors with the highest associated absolute eigenvalues are those that account for the most variance in the data. As a result, when building the PCA lower-dimensional data, we choose the $k$ principal components with the highest associated absolute eigenvalues, in non-increasing value order. By normalizing the vector of absolute eigenvalues with the L-1 norm, we obtain, for each feature, the percentage of the overall variance expained by that feature. Then, we obtain the percent variance expained by the chosen set of features by suming up the individual percent values for the chosen features. The vector of eigenvalues can also be easily recovered from the sigular values obtained from the Singular Value Decomposition (SVD) of the original centered matrix.\n",
    "\n",
    "### Data pre-processing\n",
    "\n",
    "Standardization makes features in the original feature space be compatible with each other with regards to the measurement scale. This is important in many Data Mining and Machine Learning analyses, and especially for the PCA, which aims to preserve variance. If there is significant difference in measurement scales between features (e.g., one feature is measured in mm and all others in m), the transformation will mainly pick up on the variance produced by some of the features and miss out of the more minute differences in the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1     2     3     4    5     6     7     8     9          10  \\\n",
      "0     1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29   5.640000   \n",
      "1     1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28   4.380000   \n",
      "2     1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81   5.680000   \n",
      "3     1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18   7.800000   \n",
      "4     1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82   4.320000   \n",
      "5     1  14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97   6.750000   \n",
      "6     1  14.39  1.87  2.45  14.6   96  2.50  2.52  0.30  1.98   5.250000   \n",
      "7     1  14.06  2.15  2.61  17.6  121  2.60  2.51  0.31  1.25   5.050000   \n",
      "8     1  14.83  1.64  2.17  14.0   97  2.80  2.98  0.29  1.98   5.200000   \n",
      "9     1  13.86  1.35  2.27  16.0   98  2.98  3.15  0.22  1.85   7.220000   \n",
      "10    1  14.10  2.16  2.30  18.0  105  2.95  3.32  0.22  2.38   5.750000   \n",
      "11    1  14.12  1.48  2.32  16.8   95  2.20  2.43  0.26  1.57   5.000000   \n",
      "12    1  13.75  1.73  2.41  16.0   89  2.60  2.76  0.29  1.81   5.600000   \n",
      "13    1  14.75  1.73  2.39  11.4   91  3.10  3.69  0.43  2.81   5.400000   \n",
      "14    1  14.38  1.87  2.38  12.0  102  3.30  3.64  0.29  2.96   7.500000   \n",
      "15    1  13.63  1.81  2.70  17.2  112  2.85  2.91  0.30  1.46   7.300000   \n",
      "16    1  14.30  1.92  2.72  20.0  120  2.80  3.14  0.33  1.97   6.200000   \n",
      "17    1  13.83  1.57  2.62  20.0  115  2.95  3.40  0.40  1.72   6.600000   \n",
      "18    1  14.19  1.59  2.48  16.5  108  3.30  3.93  0.32  1.86   8.700000   \n",
      "19    1  13.64  3.10  2.56  15.2  116  2.70  3.03  0.17  1.66   5.100000   \n",
      "20    1  14.06  1.63  2.28  16.0  126  3.00  3.17  0.24  2.10   5.650000   \n",
      "21    1  12.93  3.80  2.65  18.6  102  2.41  2.41  0.25  1.98   4.500000   \n",
      "22    1  13.71  1.86  2.36  16.6  101  2.61  2.88  0.27  1.69   3.800000   \n",
      "23    1  12.85  1.60  2.52  17.8   95  2.48  2.37  0.26  1.46   3.930000   \n",
      "24    1  13.50  1.81  2.61  20.0   96  2.53  2.61  0.28  1.66   3.520000   \n",
      "25    1  13.05  2.05  3.22  25.0  124  2.63  2.68  0.47  1.92   3.580000   \n",
      "26    1  13.39  1.77  2.62  16.1   93  2.85  2.94  0.34  1.45   4.800000   \n",
      "27    1  13.30  1.72  2.14  17.0   94  2.40  2.19  0.27  1.35   3.950000   \n",
      "28    1  13.87  1.90  2.80  19.4  107  2.95  2.97  0.37  1.76   4.500000   \n",
      "29    1  14.02  1.68  2.21  16.0   96  2.65  2.33  0.26  1.98   4.700000   \n",
      "..   ..    ...   ...   ...   ...  ...   ...   ...   ...   ...        ...   \n",
      "148   3  13.32  3.24  2.38  21.5   92  1.93  0.76  0.45  1.25   8.420000   \n",
      "149   3  13.08  3.90  2.36  21.5  113  1.41  1.39  0.34  1.14   9.400000   \n",
      "150   3  13.50  3.12  2.62  24.0  123  1.40  1.57  0.22  1.25   8.600000   \n",
      "151   3  12.79  2.67  2.48  22.0  112  1.48  1.36  0.24  1.26  10.800000   \n",
      "152   3  13.11  1.90  2.75  25.5  116  2.20  1.28  0.26  1.56   7.100000   \n",
      "153   3  13.23  3.30  2.28  18.5   98  1.80  0.83  0.61  1.87  10.520000   \n",
      "154   3  12.58  1.29  2.10  20.0  103  1.48  0.58  0.53  1.40   7.600000   \n",
      "155   3  13.17  5.19  2.32  22.0   93  1.74  0.63  0.61  1.55   7.900000   \n",
      "156   3  13.84  4.12  2.38  19.5   89  1.80  0.83  0.48  1.56   9.010000   \n",
      "157   3  12.45  3.03  2.64  27.0   97  1.90  0.58  0.63  1.14   7.500000   \n",
      "158   3  14.34  1.68  2.70  25.0   98  2.80  1.31  0.53  2.70  13.000000   \n",
      "159   3  13.48  1.67  2.64  22.5   89  2.60  1.10  0.52  2.29  11.750000   \n",
      "160   3  12.36  3.83  2.38  21.0   88  2.30  0.92  0.50  1.04   7.650000   \n",
      "161   3  13.69  3.26  2.54  20.0  107  1.83  0.56  0.50  0.80   5.880000   \n",
      "162   3  12.85  3.27  2.58  22.0  106  1.65  0.60  0.60  0.96   5.580000   \n",
      "163   3  12.96  3.45  2.35  18.5  106  1.39  0.70  0.40  0.94   5.280000   \n",
      "164   3  13.78  2.76  2.30  22.0   90  1.35  0.68  0.41  1.03   9.580000   \n",
      "165   3  13.73  4.36  2.26  22.5   88  1.28  0.47  0.52  1.15   6.620000   \n",
      "166   3  13.45  3.70  2.60  23.0  111  1.70  0.92  0.43  1.46  10.680000   \n",
      "167   3  12.82  3.37  2.30  19.5   88  1.48  0.66  0.40  0.97  10.260000   \n",
      "168   3  13.58  2.58  2.69  24.5  105  1.55  0.84  0.39  1.54   8.660000   \n",
      "169   3  13.40  4.60  2.86  25.0  112  1.98  0.96  0.27  1.11   8.500000   \n",
      "170   3  12.20  3.03  2.32  19.0   96  1.25  0.49  0.40  0.73   5.500000   \n",
      "171   3  12.77  2.39  2.28  19.5   86  1.39  0.51  0.48  0.64   9.899999   \n",
      "172   3  14.16  2.51  2.48  20.0   91  1.68  0.70  0.44  1.24   9.700000   \n",
      "173   3  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06   7.700000   \n",
      "174   3  13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41   7.300000   \n",
      "175   3  13.27  4.28  2.26  20.0  120  1.59  0.69  0.43  1.35  10.200000   \n",
      "176   3  13.17  2.59  2.37  20.0  120  1.65  0.68  0.53  1.46   9.300000   \n",
      "177   3  14.13  4.10  2.74  24.5   96  2.05  0.76  0.56  1.35   9.200000   \n",
      "\n",
      "       11    12    13  \n",
      "0    1.04  3.92  1065  \n",
      "1    1.05  3.40  1050  \n",
      "2    1.03  3.17  1185  \n",
      "3    0.86  3.45  1480  \n",
      "4    1.04  2.93   735  \n",
      "5    1.05  2.85  1450  \n",
      "6    1.02  3.58  1290  \n",
      "7    1.06  3.58  1295  \n",
      "8    1.08  2.85  1045  \n",
      "9    1.01  3.55  1045  \n",
      "10   1.25  3.17  1510  \n",
      "11   1.17  2.82  1280  \n",
      "12   1.15  2.90  1320  \n",
      "13   1.25  2.73  1150  \n",
      "14   1.20  3.00  1547  \n",
      "15   1.28  2.88  1310  \n",
      "16   1.07  2.65  1280  \n",
      "17   1.13  2.57  1130  \n",
      "18   1.23  2.82  1680  \n",
      "19   0.96  3.36   845  \n",
      "20   1.09  3.71   780  \n",
      "21   1.03  3.52   770  \n",
      "22   1.11  4.00  1035  \n",
      "23   1.09  3.63  1015  \n",
      "24   1.12  3.82   845  \n",
      "25   1.13  3.20   830  \n",
      "26   0.92  3.22  1195  \n",
      "27   1.02  2.77  1285  \n",
      "28   1.25  3.40   915  \n",
      "29   1.04  3.59  1035  \n",
      "..    ...   ...   ...  \n",
      "148  0.55  1.62   650  \n",
      "149  0.57  1.33   550  \n",
      "150  0.59  1.30   500  \n",
      "151  0.48  1.47   480  \n",
      "152  0.61  1.33   425  \n",
      "153  0.56  1.51   675  \n",
      "154  0.58  1.55   640  \n",
      "155  0.60  1.48   725  \n",
      "156  0.57  1.64   480  \n",
      "157  0.67  1.73   880  \n",
      "158  0.57  1.96   660  \n",
      "159  0.57  1.78   620  \n",
      "160  0.56  1.58   520  \n",
      "161  0.96  1.82   680  \n",
      "162  0.87  2.11   570  \n",
      "163  0.68  1.75   675  \n",
      "164  0.70  1.68   615  \n",
      "165  0.78  1.75   520  \n",
      "166  0.85  1.56   695  \n",
      "167  0.72  1.75   685  \n",
      "168  0.74  1.80   750  \n",
      "169  0.67  1.92   630  \n",
      "170  0.66  1.83   510  \n",
      "171  0.57  1.63   470  \n",
      "172  0.62  1.71   660  \n",
      "173  0.64  1.74   740  \n",
      "174  0.70  1.56   750  \n",
      "175  0.59  1.56   835  \n",
      "176  0.60  1.62   840  \n",
      "177  0.61  1.60   560  \n",
      "\n",
      "[178 rows x 14 columns]\n",
      "[[  1.42300000e+01   1.71000000e+00   2.43000000e+00 ...,   1.04000000e+00\n",
      "    3.92000000e+00   1.06500000e+03]\n",
      " [  1.32000000e+01   1.78000000e+00   2.14000000e+00 ...,   1.05000000e+00\n",
      "    3.40000000e+00   1.05000000e+03]\n",
      " [  1.31600000e+01   2.36000000e+00   2.67000000e+00 ...,   1.03000000e+00\n",
      "    3.17000000e+00   1.18500000e+03]\n",
      " ..., \n",
      " [  1.32700000e+01   4.28000000e+00   2.26000000e+00 ...,   5.90000000e-01\n",
      "    1.56000000e+00   8.35000000e+02]\n",
      " [  1.31700000e+01   2.59000000e+00   2.37000000e+00 ...,   6.00000000e-01\n",
      "    1.62000000e+00   8.40000000e+02]\n",
      " [  1.41300000e+01   4.10000000e+00   2.74000000e+00 ...,   6.10000000e-01\n",
      "    1.60000000e+00   5.60000000e+02]]\n",
      "[[ 1.51861254 -0.5622498   0.23205254 ...,  0.36217728  1.84791957\n",
      "   1.01300893]\n",
      " [ 0.24628963 -0.49941338 -0.82799632 ...,  0.40605066  1.1134493\n",
      "   0.96524152]\n",
      " [ 0.19687903  0.02123125  1.10933436 ...,  0.31830389  0.78858745\n",
      "   1.39514818]\n",
      " ..., \n",
      " [ 0.33275817  1.74474449 -0.38935541 ..., -1.61212515 -1.48544548\n",
      "   0.28057537]\n",
      " [ 0.20923168  0.22769377  0.01273209 ..., -1.56825176 -1.40069891\n",
      "   0.29649784]\n",
      " [ 1.39508604  1.58316512  1.36520822 ..., -1.52437837 -1.42894777\n",
      "  -0.59516041]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# read in the dataset\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='data/wine.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "    \n",
    "print(df)\n",
    "    \n",
    "# extract the vectors from the Pandas data file\n",
    "X = df.iloc[:,1:].values\n",
    "\n",
    "print(X)\n",
    "# standardise the data\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the PCA computation methods require that the data be centered, i.e., the mean of all the sample values for the jth feature is subtracted from all the jth feature sample values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subtract the mean vector from each vector in the dataset\n",
    "means = np.mean(X_std, axis=0)\n",
    "X_sm = X_std - means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1: Computing PCA via the covariance matrix\n",
    "\n",
    "One way to find the principal components is by an eigendecomposition of the covariance matrix $X_{cov} = \\frac{1}{n-1} X^TX$, where $X$ is the centered matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_cov = X_sm.T.dot(X_sm) / (X_sm.shape[0] - 1)\n",
    "\n",
    "# Side-note: Numpy has a function for computing the covariance matrix\n",
    "X_cov2 = np.cov(X_std.T)\n",
    "print(\"X_cov == X_cov2: \", np.allclose(X_cov, X_cov2))\n",
    "\n",
    "# perform the eigendecomposition of the covariance matrix\n",
    "eig_vals, eig_vecs = np.linalg.eig(X_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What remains now is to pick the eigenvectors (columns in *eig_vecs*) associated with the eigenvalues in *eig_vals* with the highest absolute values. Let's see first the percent variance expained by each eigenvalue-eigenvector pair. To do this, we sort the absolute eigenvalues and transform the values into percentages by performing L-1 normalization. We then perform a prefix-sum operation on the vector of percentages. The resulting vector will show us, in its $j$th dimension, the percent of explained variance in the PCA dimensionality reduction using $j$ dimensions. We will create a function that we can reuse to do this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percvar(v):\n",
    "    r\"\"\"Transform eigen/singular values into percents.\n",
    "    Return: vector of percents, prefix vector of percents\n",
    "    \"\"\"\n",
    "    # sort values\n",
    "    s = np.sort(np.abs(v))\n",
    "    # reverse sorting order\n",
    "    s = s[::-1]\n",
    "    # normalize\n",
    "    s = s/np.sum(s)\n",
    "    return s, np.cumsum(s)\n",
    "print(\"eigenvalues:    \", eig_vals)\n",
    "pct, pv = percvar(eig_vals)\n",
    "print(\"percent values: \", pct)\n",
    "print(\"prefix vector:  \", pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Plot the `pct` and `pv` vectors and observe the general trend of the variance as more and more dimensions are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot feature and overall percent variance\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given an expected percent variance $p$, we choose the number of features $k$ with at least that percent explained variance value in the vector $pv$, i.e., the first dimension whose value is greater or equal to the desired percent. \n",
    "\n",
    "## Exercise 2\n",
    "Create a function that, given the overall percent varience vector plotted in the previous exercise and the expected percent variance $p$, returns the number of latent space dimensions that account for $p$% variance in the data. Print out the number of dimensions for $p \\in \\{40, 60, 80, 90, 95\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perck(s, p):\n",
    "    pass\n",
    "    return len(s)\n",
    "\n",
    "for p in [40, 60, 80, 90, 95]:\n",
    "    print(\"Number of dimensions to account for %d%% of the variance: %d\" % (p, perck(pv, p*0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2: Computing PCA via the Singular Value Decomposition (SVD)\n",
    "\n",
    "We can instead compute the PCA trasformation via the SVD of the centered matrix $X = X_{sm}$. However, we will then need to transform the singular values of $X$ into eigenvalues of $X^TX$ before constructing the percent vector. In general, the non-zero singular values of a matrix $X$ are the square roots of the eigenvalues of the square matrix $X^TX$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U,s,V = np.linalg.svd(X_sm)\n",
    "# singular values of X are the square roots of the eigenvalues of the square matrix X^TX\n",
    "print(\"singular values:        \", s)\n",
    "print(\"eigenvalues:            \", (np.sort(np.abs(eig_vals)))[::-1])\n",
    "print(\"scaled singular values: \", (s**2/(X_sm.shape[0]-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since L-1 normalization is invariant to scaling by a constant factor, we can simply apply the *percvar* function to the squared singular values. The result will be equivalent to the one from Algorithm 1.\n",
    "\n",
    "**Note:** Applying the same technique directly to singular values does not give the same result. In practice, you should base your choice of $k$ on the absolute eigenvalues, which can be theoretically explained as a measure of latent variance in the feature space.\n",
    "\n",
    "## Exercise 3\n",
    "Use the `percvar` function to verify that the analysis applied to squared singular values gives the same results as the one based on the covariance matrix. Additionally, verify that the analysis based on absolute singular values does not provide the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
